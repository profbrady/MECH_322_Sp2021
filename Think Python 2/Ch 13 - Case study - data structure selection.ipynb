{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### The material in this document is copied from or based on the the book...\n",
    "\n",
    "# Think Python\n",
    "## How to Think Like a Computer Scientist\n",
    "Second edition, Version 2.2.20\n",
    "\n",
    "##### Allen Downey\n",
    "\n",
    "##### Green Tea Press\n",
    "\n",
    "Needham, Massachusetts\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Copyright © 2015 Allen Downey.\n",
    "\n",
    "Green Tea Press 9 Washburn Ave Needham MA 02492\n",
    "\n",
    "Permission is granted to copy, distribute, and/or modify this document under the terms of the Creative Commons Attribution-NonCommercial 3.0 Unported License, which is available at http://creativecommons.org/licenses/by-nc/3.0/.\n",
    "\n",
    "The original form of this book is $\\LaTeX$ source code. Compiling this $\\LaTeX$ source has the effect of generating a device-independent representation of a textbook, which can be converted to other formats and printed.\n",
    "\n",
    "The $\\LaTeX$ source for this book is available from http://www.thinkpython2.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Chapter 13 - Case study: data structure selection\n",
    "\n",
    "At this point you have learned about Python’s core data structures, and you have seen some of the algorithms that use them. If you would like to know more about algorithms, this might be a good time to read Chapter B. But you don’t have to read it before you go on; you can read it whenever you are interested.\n",
    "\n",
    "This chapter presents a case study with exercises that let you think about choosing data structures and practice using them.\n",
    "\n",
    "## 13.1 Word frequency analysis\n",
    "\n",
    "As usual, you should at least attempt the exercises before you read my solutions.\n",
    "\n",
    "**Exercise 13.1.** Write a program that reads a ﬁle, breaks each line into words, strips whitespace and punctuation from the words, and converts them to lowercase.\n",
    "\n",
    "Hint: The `string` module provides a string named `whitespace`, which contains space, tab, newline, etc., and `punctuation` which contains the punctuation characters. Let’s see if we can make Python swear:\n",
    "\n",
    "    >>> import string\n",
    "    >>> string.punctuation\n",
    "    '!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "Also, you might consider using the string methods `strip`, `replace` and `translate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 13.2.** Go to Project Gutenberg (http://gutenberg.org ) and download your favorite out-of-copyright book in plain text format.\n",
    "\n",
    "Modify your program from the previous exercise to read the book you downloaded, skip over the header information at the beginning of the ﬁle, and process the rest of the words as before.\n",
    "\n",
    "Then modify the program to count the total number of words in the book, and the number of times each word is used.\n",
    "\n",
    "Print the number of different words used in the book. Compare different books by different authors, written in different eras. Which author uses the most extensive vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 13.3.** Modify the program from the previous exercise to print the 20 most frequently used words in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 13.4.** Modify the previous program to read a word list (see Section 9.1) and then print all the words in the book that are not in the word list. How many of them are typos? How many of them are common words that should be in the word list, and how many of them are really obscure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.2 Random numbers\n",
    "\n",
    "Given the same inputs, most computer programs generate the same outputs every time, so they are said to be **deterministic**. Determinism is usually a good thing, since we expect the same calculation to yield the same result. For some applications, though, we want the computer to be unpredictable. Games are an obvious example, but there are more.\n",
    "\n",
    "Making a program truly nondeterministic turns out to be difﬁcult, but there are ways to make it at least seem nondeterministic. One of them is to use algorithms that generate **pseudorandom** numbers. Pseudorandom numbers are not truly random because they are generated by a deterministic computation, but just by looking at the numbers it is all but impossible to distinguish them from random.\n",
    "\n",
    "The `random` module provides functions that generate pseudorandom numbers (which I will simply call “random” from here on).\n",
    "\n",
    "The function `random` returns a random ﬂoat between 0.0 and 1.0 (including 0.0 but not 1.0). Each time you call `random`, you get the next number in a long series. To see a sample, run this loop:\n",
    "\n",
    "    import random\n",
    "\n",
    "    for i in range(10):\n",
    "        x = random.random()\n",
    "        print(x)\n",
    "\n",
    "The function `randint` takes parameters `low` and `high` and returns an integer between `low` and `high` (including both).\n",
    "\n",
    "    >>> random.randint(5, 10)\n",
    "    5\n",
    "    >>> random.randint(5, 10)\n",
    "    9\n",
    "\n",
    "To choose an element from a sequence at random, you can use `choice`:\n",
    "\n",
    "    >>> t = [1, 2, 3]\n",
    "    >>> random.choice(t)\n",
    "    2\n",
    "    >>> random.choice(t)\n",
    "    3\n",
    "\n",
    "The `random` module also provides functions to generate random values from continuous distributions including Gaussian, exponential, gamma, and a few more.\n",
    "\n",
    "**Exercise 13.5.** Write a function named `choose_from_hist` that takes a histogram as deﬁned in Section 11.2 and returns a random value from the histogram, chosen with probability in proportion to frequency. For example, for this histogram:\n",
    "\n",
    "    >>> t = ['a', 'a', 'b']\n",
    "    >>> hist = histogram(t)\n",
    "    >>> hist\n",
    "    {'a': 2, 'b': 1}\n",
    "\n",
    "your function should return `'a'` with probability 2/3 and `'b'` with probability 1/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.3 Word histogram\n",
    "\n",
    "You should attempt the previous exercises before you go on. You can download my solution from http://thinkpython2.com/code/analyze_book1.py. You will also need http://thinkpython2.com/code/emma.txt.\n",
    "\n",
    "Here is a program that reads a ﬁle and builds a histogram of the words in the ﬁle:\n",
    "\n",
    "    import string\n",
    "\n",
    "    def process_file(filename):\n",
    "        hist = dict()\n",
    "        fp = open(filename)\n",
    "        for line in fp:\n",
    "            process_line(line, hist)\n",
    "        return hist\n",
    "\n",
    "    def process_line(line, hist):\n",
    "        line = line.replace('-', ' ')\n",
    "        for word in line.split():\n",
    "            word = word.strip(string.punctuation + string.whitespace)\n",
    "            word = word.lower()\n",
    "            hist[word] = hist.get(word, 0) + 1\n",
    "\n",
    "    hist = process_file('emma.txt')\n",
    "\n",
    "This program reads `emma.txt`, which contains the text of *Emma* by Jane Austen. `process_file` loops through the lines of the ﬁle, passing them one at a time to `process_line`. The histogram `hist` is being used as an accumulator.\n",
    "\n",
    "`process_line` uses the string method `replace` to replace hyphens with spaces before using `split` to break the line into a list of strings. It traverses the list of words and uses `strip` and `lower` to remove punctuation and convert to lower case. (It is a shorthand to say that strings are “converted”; remember that strings are immutable, so methods like `strip` and `lower` return new strings.)\n",
    "\n",
    "Finally, `process_line` updates the histogram by creating a new item or incrementing an existing one.\n",
    "\n",
    "To count the total number of words in the ﬁle, we can add up the frequencies in the histogram:\n",
    "\n",
    "    def total_words(hist):\n",
    "        return sum(hist.values())\n",
    "\n",
    "The number of different words is just the number of items in the dictionary:\n",
    "\n",
    "    def different_words(hist):\n",
    "        return len(hist)\n",
    "\n",
    "Here is some code to print the results:\n",
    "\n",
    "    print('Total number of words:', total_words(hist))\n",
    "    print('Number of different words:', different_words(hist))\n",
    "\n",
    "And the results:\n",
    "\n",
    "    Total number of words: 161080\n",
    "    Number of different words: 7214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.4 Most common words\n",
    "\n",
    "To ﬁnd the most common words, we can make a list of tuples, where each tuple contains a word and its frequency, and sort it.\n",
    "\n",
    "The following function takes a histogram and returns a list of word-frequency tuples:\n",
    "\n",
    "    def most_common(hist):\n",
    "        t = []\n",
    "        for key, value in hist.items():\n",
    "            t.append((value, key))\n",
    "\n",
    "        t.sort(reverse=True)\n",
    "        return t\n",
    "\n",
    "In each tuple, the frequency appears ﬁrst, so the resulting list is sorted by frequency. Here is a loop that prints the ten most common words:\n",
    "\n",
    "    t = most_common(hist)\n",
    "    print('The most common words are:')\n",
    "    for freq, word in t[:10]:\n",
    "        print(word, freq, sep='\\t')\n",
    "\n",
    "I use the keyword argument `sep` to tell `print` to use a tab character as a “separator”, rather than a space, so the second column is lined up. Here are the results from *Emma*:\n",
    "\n",
    "    The most common words are:\n",
    "    to      5242\n",
    "    the     5205\n",
    "    and     4897\n",
    "    of      4295\n",
    "    i       3191\n",
    "    a       3130\n",
    "    it      2529\n",
    "    her     2483\n",
    "    was     2400\n",
    "    she     2364\n",
    "\n",
    "This code can be simpliﬁed using the key parameter of the sort function. If you are curious, you can read about it at https://wiki.python.org/moin/HowTo/Sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.5 Optional parameters\n",
    "\n",
    "We have seen built-in functions and methods that take optional arguments. It is possible to write programmer-deﬁned functions with optional arguments, too. For example, here is a function that prints the most common words in a histogram\n",
    "\n",
    "    def print_most_common(hist, num=10):\n",
    "        t = most_common(hist)\n",
    "        print('The most common words are:')\n",
    "        for freq, word in t[:num]:\n",
    "            print(word, freq, sep='\\t')\n",
    "\n",
    "The ﬁrst parameter is required; the second is optional. The **default value** of `num` is 10.\n",
    "\n",
    "If you only provide one argument:\n",
    "\n",
    "    print_most_common(hist)\n",
    "\n",
    "`num` gets the default value. If you provide two arguments:\n",
    "\n",
    "    print_most_common(hist, 20)\n",
    "\n",
    "`num` gets the value of the argument instead. In other words, the optional argument overrides the default value.\n",
    "\n",
    "If a function has both required and optional parameters, all the required parameters have to come ﬁrst, followed by the optional ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.6 Dictionary subtraction\n",
    "\n",
    "Finding the words from the book that are not in the word list from `words.txt` is a problem you might recognize as set subtraction; that is, we want to ﬁnd all the words from one set (the words in the book) that are not in the other (the words in the list).\n",
    "\n",
    "`subtract` takes dictionaries `d1` and `d2` and returns a new dictionary that contains all the keys from `d1` that are not in `d2`. Since we don’t really care about the values, we set them all to `None`.\n",
    "\n",
    "    def subtract(d1, d2):\n",
    "        res = dict()\n",
    "        for key in d1:\n",
    "            if key not in d2:\n",
    "                res[key] = None\n",
    "        return res\n",
    "\n",
    "To ﬁnd the words in the book that are not in `words.txt`, we can use `process_file` to build a histogram for `words.txt`, and then subtract:\n",
    "\n",
    "    words = process_file('words.txt')\n",
    "    diff = subtract(hist, words)\n",
    "\n",
    "    print(\"Words in the book that aren't in the word list:\")\n",
    "    for word in diff:\n",
    "        print(word, end=' ')\n",
    "\n",
    "Here are some of the results from *Emma*:\n",
    "\n",
    "    Words in the book that aren't in the word list:\n",
    "    rencontre jane's blanche woodhouses disingenuousness\n",
    "    friend's venice apartment ...\n",
    "\n",
    "Some of these words are names and possessives. Others, like “rencontre”, are no longer in common use. But a few are common words that should really be in the list!\n",
    "\n",
    "**Exercise 13.6.** Python provides a data structure called `set` that provides many common set operations. You can read about them in Section 19.5, or read the documentation at http://docs.python.org/3/library/stdtypes.html#types-set.\n",
    "\n",
    "Write a program that uses set subtraction to ﬁnd words in the book that are not in the word list. Solution: http://thinkpython2.com/code/analyze_book2.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.7 Random words\n",
    "\n",
    "To choose a random word from the histogram, the simplest algorithm is to build a list with multiple copies of each word, according to the observed frequency, and then choose from the list:\n",
    "\n",
    "    def random_word(h):\n",
    "        t = []\n",
    "        for word, freq in h.items():\n",
    "            t.extend([word] * freq)\n",
    "        return random.choice(t)\n",
    "\n",
    "The expression `[word] * freq` creates a list with `freq` copies of the string word. The `extend` method is similar to `append` except that the argument is a sequence.\n",
    "\n",
    "This algorithm works, but it is not very efﬁcient; each time you choose a random word, it rebuilds the list, which is as big as the original book. An obvious improvement is to build the list once and then make multiple selections, but the list is still big.\n",
    "\n",
    "An alternative is:\n",
    "\n",
    "1. Use keys to get a list of the words in the book.\n",
    "\n",
    "2. Build a list that contains the cumulative sum of the word frequencies (see Exercise 10.2). The last item in this list is the total number of words in the book, n.\n",
    "\n",
    "3. Choose a random number from 1 to $n$. Use a bisection search (See Exercise 10.10) to ﬁnd the index where the random number would be inserted in the cumulative sum.\n",
    "\n",
    "4. Use the index to ﬁnd the corresponding word in the word list.\n",
    "\n",
    "**Exercise 13.7.** Write a program that uses this algorithm to choose a random word from the book. Solution: http://thinkpython2.com/code/analyze_book3.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.8 Markov analysis\n",
    "\n",
    "If you choose words from the book at random, you can get a sense of the vocabulary, but you probably won’t get a sentence:\n",
    "\n",
    "    this the small regard harriet which knightley's it most things\n",
    "\n",
    "A series of random words seldom makes sense because there is no relationship between successive words. For example, in a real sentence you would expect an article like “the” to be followed by an adjective or a noun, and probably not a verb or adverb.\n",
    "\n",
    "One way to measure these kinds of relationships is Markov analysis, which characterizes, for a given sequence of words, the probability of the words that might come next. For example, the song *Eric, the Half a Bee* begins:\n",
    "\n",
    ">Half a bee, philosophically,\n",
    "\n",
    ">Must, ipso facto, half not be.\n",
    "\n",
    ">But half the bee has got to be\n",
    "\n",
    ">Vis a vis, its entity. D’you see?\n",
    "\n",
    ">&nbsp;\n",
    "\n",
    ">But can a bee be said to be\n",
    "\n",
    ">Or not to be an entire bee\n",
    "\n",
    ">When half the bee is not a bee\n",
    "\n",
    ">Due to some ancient injury?\n",
    "\n",
    "In this text, the phrase “half the” is always followed by the word “bee”, but the phrase “the bee” might be followed by either “has” or “is”.\n",
    "\n",
    "The result of Markov analysis is a mapping from each preﬁx (like “half the” and “the bee”) to all possible sufﬁxes (like “has” and “is”).\n",
    "\n",
    "Given this mapping, you can generate a random text by starting with any preﬁx and choosing at random from the possible sufﬁxes. Next, you can combine the end of the preﬁx and the new sufﬁx to form the next preﬁx, and repeat.\n",
    "\n",
    "For example, if you start with the preﬁx “Half a”, then the next word has to be “bee”, because the preﬁx only appears once in the text. The next preﬁx is “a bee”, so the next sufﬁx might be “philosophically”, “be” or “due”.\n",
    "\n",
    "In this example the length of the preﬁx is always two, but you can do Markov analysis with any preﬁx length.\n",
    "\n",
    "**Exercise 13.8.** Markov analysis:\n",
    "\n",
    "1. Write a program to read a text from a ﬁle and perform Markov analysis. The result should be a dictionary that maps from preﬁxes to a collection of possible sufﬁxes. The collection might be a list, tuple, or dictionary; it is up to you to make an appropriate choice. You can test your program with preﬁx length two, but you should write the program in a way that makes it easy to try other lengths.\n",
    "\n",
    "2. Add a function to the previous program to generate random text based on the Markov analysis.\n",
    "\n",
    "  Here is an example from *Emma* with preﬁx length 2:\n",
    "\n",
    "  >He was very clever, be it sweetness or be angry, ashamed or only amused, at such a stroke. She had never thought of Hannah till you were never meant for me?\" \"I cannot make speeches, Emma:\" he soon cut it all himself.\n",
    "\n",
    "  For this example, I left the punctuation attached to the words. The result is almost syntactically correct, but not quite. Semantically, it almost makes sense, but not quite.\n",
    "\n",
    "  What happens if you increase the preﬁx length? Does the random text make more sense?\n",
    "\n",
    "3. Once your program is working, you might want to try a mash-up: if you combine text from two or more books, the random text you generate will blend the vocabulary and phrases from the sources in interesting ways.\n",
    "\n",
    "Credit: This case study is based on an example from Kernighan and Pike, *The Practice of Programming*, Addison-Wesley, 1999.\n",
    "\n",
    "You should attempt this exercise before you go on; then you can can download my solution from http://thinkpython2.com/code/markov.py. You will also need http: //thinkpython2.com/code/emma.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.9 Data structures\n",
    "\n",
    "Using Markov analysis to generate random text is fun, but there is also a point to this exercise: data structure selection. In your solution to the previous exercises, you had to choose:\n",
    "\n",
    "* How to represent the preﬁxes.\n",
    "\n",
    "* How to represent the collection of possible sufﬁxes.\n",
    "\n",
    "* How to represent the mapping from each preﬁx to the collection of possible sufﬁxes.\n",
    "\n",
    "The last one is easy: a dictionary is the obvious choice for a mapping from keys to corresponding values.\n",
    "\n",
    "For the preﬁxes, the most obvious options are string, list of strings, or tuple of strings.\n",
    "\n",
    "For the sufﬁxes, one option is a list; another is a histogram (dictionary).\n",
    "\n",
    "How should you choose? The ﬁrst step is to think about the operations you will need to implement for each data structure. For the preﬁxes, we need to be able to remove words from the beginning and add to the end. For example, if the current preﬁx is “Half a”, and the next word is “bee”, you need to be able to form the next preﬁx, “a bee”.\n",
    "\n",
    "Your ﬁrst choice might be a list, since it is easy to add and remove elements, but we also need to be able to use the preﬁxes as keys in a dictionary, so that rules out lists. With tuples, you can’t append or remove, but you can use the addition operator to form a new tuple:\n",
    "\n",
    "    def shift(prefix, word):\n",
    "        return prefix[1:] + (word,)\n",
    "\n",
    "`shift` takes a tuple of words, `prefix`, and a string, word, and forms a new tuple that has all the words in `prefix` except the ﬁrst, and `word` added to the end.\n",
    "\n",
    "For the collection of sufﬁxes, the operations we need to perform include adding a new sufﬁx (or increasing the frequency of an existing one), and choosing a random sufﬁx.\n",
    "\n",
    "Adding a new sufﬁx is equally easy for the list implementation or the histogram. Choosing a random element from a list is easy; choosing from a histogram is harder to do efﬁciently (see Exercise 13.7).\n",
    "\n",
    "So far we have been talking mostly about ease of implementation, but there are other factors to consider in choosing data structures. One is run time. Sometimes there is a theoretical reason to expect one data structure to be faster than other; for example, I mentioned that the `in` operator is faster for dictionaries than for lists, at least when the number of elements is large.\n",
    "\n",
    "But often you don’t know ahead of time which implementation will be faster. One option is to implement both of them and see which is better. This approach is called **benchmarking**. A practical alternative is to choose the data structure that is easiest to implement, and then see if it is fast enough for the intended application. If so, there is no need to go on. If not, there are tools, like the `profile` module, that can identify the places in a program that take the most time.\n",
    "\n",
    "The other factor to consider is storage space. For example, using a histogram for the collection of sufﬁxes might take less space because you only have to store each word once, no matter how many times it appears in the text. In some cases, saving space can also make your program run faster, and in the extreme, your program might not run at all if you run out of memory. But for many applications, space is a secondary consideration after run time.\n",
    "\n",
    "One ﬁnal thought: in this discussion, I have implied that we should use one data structure for both analysis and generation. But since these are separate phases, it would also be possible to use one structure for analysis and then convert to another structure for generation. This would be a net win if the time saved during generation exceeded the time spent in conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.10 Debugging\n",
    "\n",
    "When you are debugging a program, and especially if you are working on a hard bug, there are ﬁve things to try:\n",
    "\n",
    "**Reading:** Examine your code, read it back to yourself, and check that it says what you meant to say.\n",
    "\n",
    "**Running:** Experiment by making changes and running different versions. Often if you display the right thing at the right place in the program, the problem becomes obvious, but sometimes you have to build scaffolding.\n",
    "\n",
    "**Ruminating:** Take some time to think! What kind of error is it: syntax, runtime, or semantic? What information can you get from the error messages, or from the output of the program? What kind of error could cause the problem you’re seeing? What did you change last, before the problem appeared?\n",
    "\n",
    "**Rubberducking:** If you explain the problem to someone else, you sometimes ﬁnd the answer before you ﬁnish asking the question. Often you don’t need the other person; you could just talk to a rubber duck. And that’s the origin of the wellknown strategy called **rubber duck debugging**. I am not making this up; see https://en.wikipedia.org/wiki/Rubber_duck_debugging.\n",
    "\n",
    "**Retreating:** At some point, the best thing to do is back off, undoing recent changes, until you get back to a program that works and that you understand. Then you can start rebuilding.\n",
    "\n",
    "Beginning programmers sometimes get stuck on one of these activities and forget the others. Each activity comes with its own failure mode.\n",
    "\n",
    "For example, reading your code might help if the problem is a typographical error, but not if the problem is a conceptual misunderstanding. If you don’t understand what your program does, you can read it 100 times and never see the error, because the error is in your head.\n",
    "\n",
    "Running experiments can help, especially if you run small, simple tests. But if you run experiments without thinking or reading your code, you might fall into a pattern I call “random walk programming”, which is the process of making random changes until the program does the right thing. Needless to say, random walk programming can take a long time.\n",
    "\n",
    "You have to take time to think. Debugging is like an experimental science. You should have at least one hypothesis about what the problem is. If there are two or more possibilities, try to think of a test that would eliminate one of them.\n",
    "\n",
    "But even the best debugging techniques will fail if there are too many errors, or if the code you are trying to ﬁx is too big and complicated. Sometimes the best option is to retreat, simplifying the program until you get to something that works and that you understand.\n",
    "\n",
    "Beginning programmers are often reluctant to retreat because they can’t stand to delete a line of code (even if it’s wrong). If it makes you feel better, copy your program into another ﬁle before you start stripping it down. Then you can copy the pieces back one at a time.\n",
    "\n",
    "Finding a hard bug requires reading, running, ruminating, and sometimes retreating. If you get stuck on one of these activities, try the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.11 Glossary\n",
    "\n",
    "**deterministic:** Pertaining to a program that does the same thing each time it runs, given the same inputs.\n",
    "\n",
    "**pseudorandom:** Pertaining to a sequence of numbers that appears to be random, but is generated by a deterministic program.\n",
    "\n",
    "**default value:** The value given to an optional parameter if no argument is provided. override: To replace a default value with an argument.\n",
    "\n",
    "**benchmarking:** The process of choosing between data structures by implementing alternatives and testing them on a sample of the possible inputs.\n",
    "\n",
    "**rubber duck debugging:** Debugging by explaining your problem to an inanimate object such as a rubber duck. Articulating the problem can help you solve it, even if the rubber duck doesn’t know Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 13.12 Exercises\n",
    "\n",
    "**Exercise 13.9.** The “rank” of a word is its position in a list of words sorted by frequency: the most common word has rank 1, the second most common has rank 2, etc.\n",
    "\n",
    "Zipf’s law describes a relationship between the ranks and frequencies of words in natural languages (http://en.wikipedia.org/wiki/Zipf's_law). Speciﬁcally, it predicts that the frequency, $f$, of the word with rank $r$ is:\n",
    "\n",
    "$$f=cr^{-s}$$\n",
    "\n",
    "where $s$ and $c$ are parameters that depend on the language and the text. If you take the logarithm of both sides of this equation, you get:\n",
    "\n",
    "$$ \\log{f} = \\log{c} - s\\log{r} $$\n",
    "\n",
    "So if you plot $\\log{f}$ versus $\\log {r}$, you should get a straight line with slope $−s$ and intercept $\\log {c}$.\n",
    "\n",
    "Write a program that reads a text from a ﬁle, counts word frequencies, and prints one line for each word, in descending order of frequency, with $\\log{f}$ and $\\log{r}$. Use the graphing program of your choice to plot the results and check whether they form a straight line. Can you estimate the value of $s$?\n",
    "\n",
    "Solution: http://thinkpython2.com/code/zipf.py. To run my solution, you need the plotting module `matplotlib`. If you installed Anaconda, you already have `matplotlib`; otherwise you might have to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}